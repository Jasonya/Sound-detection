{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"XXXXXX.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9c2f84ad639041e68561ca817b4813ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_30e8e458896a475ea838f142c35c699c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4549ddd4bb3c461489ebcc2243171d38","IPY_MODEL_874a8fddc8244189a4d07c0a9087606d","IPY_MODEL_d319e5ec28734b9a8f9592fbced1cd14"]}},"30e8e458896a475ea838f142c35c699c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4549ddd4bb3c461489ebcc2243171d38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa9bf3a8465a45e1a77c0b318e20dadb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a8b7cfc0cac94d85886570c404d11643"}},"874a8fddc8244189a4d07c0a9087606d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fb3acd6c13af4d15a92f4b85e0d00596","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":356082095,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":356082095,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e76e94a66844402a2157470e362a0f3"}},"d319e5ec28734b9a8f9592fbced1cd14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ef59d415c5894efe8d34a72e04c40c86","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 340M/340M [00:03&lt;00:00, 95.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_059695000ef643d88a479a48435e292e"}},"aa9bf3a8465a45e1a77c0b318e20dadb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a8b7cfc0cac94d85886570c404d11643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fb3acd6c13af4d15a92f4b85e0d00596":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4e76e94a66844402a2157470e362a0f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef59d415c5894efe8d34a72e04c40c86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"059695000ef643d88a479a48435e292e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4O-SLFB4y53N","executionInfo":{"status":"ok","timestamp":1630603953804,"user_tz":240,"elapsed":21725,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"d5045963-1e78-4c30-faac-1867a4b902b5"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"DesMUKN4RmfJ"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-irKtLP9RUo","executionInfo":{"status":"ok","timestamp":1630604042214,"user_tz":240,"elapsed":2431,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"37965359-7335-4bee-e50c-1e7c760ed01d"},"source":["import torch\n","\n","USE_GPU = True\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print('using device:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cuda\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"t13H3oCn3YfY","executionInfo":{"status":"ok","timestamp":1630604046071,"user_tz":240,"elapsed":1445,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"6824be15-1ef7-48d1-aba4-60c4d7862303"},"source":["import pandas as pd\n","from pathlib import Path\n","%cd \"/content/drive/MyDrive/Colab Notebooks/SC201-Project/archive\"\n","download_path = Path.cwd()\n","\n","# Read metadata file\n","metadata_file = download_path/'UrbanSound8K.csv'\n","df = pd.read_csv(metadata_file)\n","df.head()\n","\n","# Construct file path by concatenating fold and file name\n","df['relative_path'] = '/fold' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str)\n","\n","# Take relevant columns\n","df = df[['relative_path', 'classID']]\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/SC201-Project/archive\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>relative_path</th>\n","      <th>classID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/fold5/100032-3-0-0.wav</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/fold5/100263-2-0-117.wav</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/fold5/100263-2-0-121.wav</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/fold5/100263-2-0-126.wav</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/fold5/100263-2-0-137.wav</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               relative_path  classID\n","0    /fold5/100032-3-0-0.wav        3\n","1  /fold5/100263-2-0-117.wav        2\n","2  /fold5/100263-2-0-121.wav        2\n","3  /fold5/100263-2-0-126.wav        2\n","4  /fold5/100263-2-0-137.wav        2"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaztifu335UG","executionInfo":{"status":"ok","timestamp":1630604054428,"user_tz":240,"elapsed":4843,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"b622f0cd-7fcd-4a99-c383-427dcaf87c14"},"source":["import math, random\n","import torch\n","! pip install torchaudio\n","import torchaudio\n","from torchaudio import transforms\n","from IPython.display import Audio\n","\n","class AudioUtil():\n","  \"\"\"\n","  The AudioUtil Class includes all the function to finish the data transform,\n","  preprocessing and augmentation\n","  \"\"\"\n","\n","  @staticmethod\n","  def open(audio_file):\n","    # Load an audio file. Return the signal as a tensor and the sample rate\n","    sig, sr = torchaudio.load(audio_file)\n","    return (sig, sr)\n","\n","\n","  @staticmethod\n","  def rechannel(aud, new_channel):\n","    # Convert the given audio to the desired number of channels\n","    sig, sr = aud\n","    if (sig.shape[0] == new_channel):\n","      # Nothing to do\n","      return aud\n","    if (new_channel == 1):\n","      # Convert from stereo to mono by selecting only the first channel\n","      resig = sig[:1, :]\n","    else:\n","      # Convert from mono to stereo by duplicating the first channel\n","      resig = torch.cat([sig, sig])\n","    return ((resig, sr))\n","\n","\n","  @staticmethod\n","  def resample(aud, newsr):\n","    # Since Resample applies to a single channel, \n","    # we resample one channel at a time\n","    sig, sr = aud\n","    if (sr == newsr):\n","      # Nothing to do\n","      return aud\n","    num_channels = sig.shape[0]\n","    # Resample first channel\n","    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n","    if (num_channels > 1):\n","      # Resample the second channel and merge both channels\n","      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n","      resig = torch.cat([resig, retwo])\n","    return ((resig, newsr))\n","\n","\n","  @staticmethod\n","  def pad_trunc(aud, max_ms):\n","    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n","    sig, sr = aud\n","    num_rows, sig_len = sig.shape\n","    max_len = sr//1000 * max_ms\n","\n","    if (sig_len > max_len):\n","      # Truncate the signal to the given length\n","      sig = sig[:,:max_len]\n","\n","    elif (sig_len < max_len):\n","      # Length of padding to add at the beginning and end of the signal\n","      pad_begin_len = random.randint(0, max_len - sig_len)\n","      pad_end_len = max_len - sig_len - pad_begin_len\n","\n","      # Pad with 0s\n","      pad_begin = torch.zeros((num_rows, pad_begin_len))\n","      pad_end = torch.zeros((num_rows, pad_end_len))\n","\n","      sig = torch.cat((pad_begin, sig, pad_end), 1)\n","    return (sig, sr)\n","\n","\n","  @staticmethod\n","  def time_shift(aud, shift_limit):\n","    # data augumentation, shifting audio to left or right\n","    sig,sr = aud\n","    _, sig_len = sig.shape\n","    shift_amt = int(random.random() * shift_limit * sig_len)\n","    return (sig.roll(shift_amt), sr)\n","\n","\n","  @staticmethod\n","  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n","    # transform the audio to the Mel Spectrogram\n","    sig,sr = aud\n","    top_db = 80\n","    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n","    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n","    # Convert to decibels\n","    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n","    return (spec)\n","\n","\n","  @staticmethod\n","  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n","    # Augment the Spectrogram by masking out some sections of it in both the frequency\n","    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n","    # overfitting and to help the model generalise better. The masked sections are\n","    # replaced with the mean value.\n","    _, n_mels, n_steps = spec.shape\n","    mask_value = spec.mean()\n","    aug_spec = spec\n","\n","    freq_mask_param = max_mask_pct * n_mels\n","    for _ in range(n_freq_masks):\n","      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n","\n","    time_mask_param = max_mask_pct * n_steps\n","    for _ in range(n_time_masks):\n","      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n","\n","    return aug_spec"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchaudio\n","  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.9.0\n"]}]},{"cell_type":"code","metadata":{"id":"8DFCE4164fAn"},"source":["from torch.utils.data import DataLoader, Dataset, random_split\n","import torchaudio\n","\n","\n","class SoundDS(Dataset):\n","  def __init__(self, df, data_path):\n","    # initialte the parameters and load teh dateset\n","    self.df = df\n","    self.data_path = str(data_path)\n","    self.duration = 4000\n","    self.sr = 44100\n","    self.channel = 2\n","    self.shift_pct = 0.4\n","            \n","  def __len__(self):\n","    # return the length of dataset\n","    return len(self.df)    \n","    \n","\n","  # Get i'th item in dataset\n","  def __getitem__(self, idx):\n","    # Absolute file path of the audio file - concatenate the audio directory with\n","    # the relative path\n","    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n","    # Get the Class ID\n","    class_id = self.df.loc[idx, 'classID']\n","    aud = AudioUtil.open(audio_file)\n","    # Some sounds have a higher sample rate, or fewer channels compared to the\n","    # majority. So make all sounds have the same number of channels and same \n","    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n","    # result in arrays of different lengths, even though the sound duration is\n","    # the same.\n","    reaud = AudioUtil.resample(aud, self.sr)\n","    rechan = AudioUtil.rechannel(reaud, self.channel)\n","\n","    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n","    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n","    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n","    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n","\n","    return aug_sgram, class_id"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxiRvY8j4pA8"},"source":["from torch.utils.data import random_split\n","\n","myds = SoundDS(df, download_path)\n","\n","# Random split of 80:20 between training and validation\n","num_items = len(myds)\n","num_train = round(num_items * 0.8)\n","num_val = num_items - num_train\n","train_ds, val_ds = random_split(myds, [num_train, num_val])\n","\n","# Create training and validation data loaders\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLw_DpkQ5Gty","executionInfo":{"status":"ok","timestamp":1630604070359,"user_tz":240,"elapsed":5791,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"2cf24dee-ecca-40ed-fa47-0b29a56e49db"},"source":["import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.nn import init\n","\n","\n","class AudioClassifier (nn.Module):\n","    # Build the model architecture\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(8)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","\n","        # Thrid Convolution Block\n","        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","\n","        # Fourth Convolution Block\n","        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=64, out_features=10)\n","\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n"," \n","\n","    # Forward pass computations\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        # Linear layer\n","        x = self.lin(x)\n","\n","        # Final output\n","        return x\n","\n","# Create the model and put it on the GPU if available\n","myModel = AudioClassifier()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","myModel = myModel.to(device)\n","# Check that it is on Cuda\n","next(myModel.parameters()).device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420,"referenced_widgets":["9c2f84ad639041e68561ca817b4813ef","30e8e458896a475ea838f142c35c699c","4549ddd4bb3c461489ebcc2243171d38","874a8fddc8244189a4d07c0a9087606d","d319e5ec28734b9a8f9592fbced1cd14","aa9bf3a8465a45e1a77c0b318e20dadb","a8b7cfc0cac94d85886570c404d11643","fb3acd6c13af4d15a92f4b85e0d00596","4e76e94a66844402a2157470e362a0f3","ef59d415c5894efe8d34a72e04c40c86","059695000ef643d88a479a48435e292e"]},"id":"wPGbxUR65TB8","executionInfo":{"status":"error","timestamp":1630604081092,"user_tz":240,"elapsed":7201,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"e6fbda2d-9896-4c97-f512-fc85fc45e5c2"},"source":["import time\n","from torchvision import models\n","validation_loss_history = []\n","validation_acc_history = []\n","training_loss_histroy = []\n","training_acc_history = []\n","# Training Loop\n","# ----------------------------\n","def training(train_dl, num_epochs, model):\n","  \"\"\"\n","  Loss Function, Optimizer and Scheduler\n","  Loss Function: For multi-class problem, Cross Entropy and KL divergence are the\n","                 loss often used.\n","  Learning Rating - OneCycleLR: Sets the learning rate of each parameter group \n","  according to the 1cycle learning rate policy. The 1cycle policy anneals the \n","  learning rate from an initial learning rate to some maximum learning rate and \n","  then from that maximum learning rate to some minimum learning rate much lower \n","  than the initial learning rate.\n","  \"\"\"\n","  \n","\n","\n","  if model == 'baseline':\n","    model_save_name = 'classifier_baseline.pt'\n","    model = myModel\n","    print(f'model={myModel}')\n","  elif model == 'VGG':\n","    model_save_name = 'classifier_vgg19bn.pt'\n","    model = models.vgg19_bn(pretrained=True).cuda()\n","    for param in model.parameters():\n","      param.requires_grad = False\n","    first_conv_layer = [nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)]\n","    first_conv_layer.extend(list(model.features))  \n","    model.features= nn.Sequential(*first_conv_layer )  \n","\n","    # add the classifier\n","    model.classifier[6] = nn.Sequential(\n","                          nn.Linear(in_features=4096, out_features=256, bias=True),\n","                          nn.ReLU(),\n","                          nn.Dropout(0.4),\n","                          nn.Linear(in_features=256, out_features=10, bias=True),\n","                          )\n","    model = model.to(device)\n","  elif model == 'ResNet':\n","    model_save_name = 'classifier_resnext101-unfreezed-100epooch.pt'\n","    model = models.resnext101_32x8d(pretrained=True).cuda()\n","\n","    model = nn.Sequential(\n","      nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n","      model)\n","    model.fc = nn.Sequential(\n","        nn.Linear(1000,10,bias=True))\n","    model = model.to(device)\n","    for param in model.parameters():\n","      param.requires_grad = True\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","\n","\n","  # Repeat for each epoch\n","  print('Started Training')\n","  st_time = time.time()\n","  for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Repeat for each batch in the training set\n","    for i, data in enumerate(train_dl):\n","        # Get the input features and target labels, and put them on the GPU\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # Normalize the inputs\n","        inputs_m, inputs_s = inputs.mean(), inputs.std()\n","        inputs = (inputs - inputs_m) / inputs_s\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Keep stats for Loss and Accuracy\n","        running_loss += loss.item()\n","\n","        # Get the predicted class with the highest score\n","        _, prediction = torch.max(outputs,1)\n","        # Count of predictions that matched the target label\n","        correct_prediction += (prediction == labels).sum().item()\n","        total_prediction += prediction.shape[0]\n","\n","        if i % 50 == 0 and i != 0:    # print every 50 mini-batches\n","           print('At epoch {}, batch {}, having loss: {:.3f}'.format(epoch + 1, i + 1, running_loss / i))\n","    \n","    # Print stats at the end of the epoch\n","    \n","    num_batches = len(train_dl)\n","    avg_loss = running_loss / num_batches\n","    acc = correct_prediction/total_prediction\n","    training_loss_histroy.append(avg_loss)\n","    training_acc_history.append(acc)\n","    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.3f}, Training Accuracy: {acc:.3f}')\n","    \n","\n","    # if (epoch+1) % 5 == 0:\n","      # check the accuracy every 5 epoch\n","    inference(model, val_dl)\n","\n","  et_time = time.time()\n","  path = F\"/content/drive/MyDrive/Colab Notebooks/SC201-Project/{model_save_name}\" \n","  torch.save(model.state_dict(), path)\n","  print('Finished Training')\n","  print('The training time is {:.3f} seconds'.format(et_time-st_time))\n","num_epochs= 50\n","training(train_dl, num_epochs, model='ResNet')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-8ba56ff5.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c2f84ad639041e68561ca817b4813ef","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/340M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Started Training\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-f97d014c5df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The training time is {:.3f} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0met_time\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ResNet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-f97d014c5df9>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(train_dl, num_epochs, model)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Repeat for each batch in the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Get the input features and target labels, and put them on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-841a8c337f1b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Get the Class ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mclass_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0maud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Some sounds have a higher sample rate, or fewer channels compared to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# majority. So make all sounds have the same number of channels and same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-807171d3f2fc>\u001b[0m in \u001b[0;36mopen\u001b[0;34m(audio_file)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Load an audio file. Return the signal as a tensor and the sample rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     return torch.ops.torchaudio.sox_io_load_audio_file(\n\u001b[0;32m--> 153\u001b[0;31m         filepath, frame_offset, num_frames, normalize, channels_first, format)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4MbRXcEcd8m","executionInfo":{"status":"ok","timestamp":1630604110837,"user_tz":240,"elapsed":5441,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"1062302a-eec9-4b73-ea9a-b0ce19eb0379"},"source":["from torchvision import models\n","model = models.resnext101_32x8d(pretrained=True).cuda()\n","for param in model.parameters():\n","  param.requires_grad = False\n","model = nn.Sequential(\n","  nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n","  model)\n","model.fc = nn.Sequential(\n","    nn.Linear(1000,10,bias=True))\n","model = model.to(device)\n","model_save_name = 'classifier_resnext101-unfreezed.pt'\n","path = F\"/content/drive/MyDrive/Colab Notebooks/SC201-Project/{model_save_name}\"\n","model.load_state_dict(torch.load(path))\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOS9t-QbBnKJ","executionInfo":{"status":"ok","timestamp":1630634260749,"user_tz":240,"elapsed":12209133,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"8678501f-88d8-4fa4-c265-8334f90f41a9"},"source":["import time\n","from torchvision import models\n","validation_loss_history = []\n","validation_acc_history = []\n","training_loss_histroy = []\n","training_acc_history = []\n","# Training Loop\n","# ----------------------------\n","def training(train_dl, num_epochs, model):\n","  \"\"\"\n","  Loss Function, Optimizer and Scheduler\n","  Loss Function: For multi-class problem, Cross Entropy and KL divergence are the\n","                 loss often used.\n","  Learning Rating - OneCycleLR: Sets the learning rate of each parameter group \n","  according to the 1cycle learning rate policy. The 1cycle policy anneals the \n","  learning rate from an initial learning rate to some maximum learning rate and \n","  then from that maximum learning rate to some minimum learning rate much lower \n","  than the initial learning rate.\n","  \"\"\"\n","  \n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","\n","\n","  # Repeat for each epoch\n","  print('Started Training')\n","  st_time = time.time()\n","  for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Repeat for each batch in the training set\n","    for i, data in enumerate(train_dl):\n","        # Get the input features and target labels, and put them on the GPU\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # Normalize the inputs\n","        inputs_m, inputs_s = inputs.mean(), inputs.std()\n","        inputs = (inputs - inputs_m) / inputs_s\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Keep stats for Loss and Accuracy\n","        running_loss += loss.item()\n","\n","        # Get the predicted class with the highest score\n","        _, prediction = torch.max(outputs,1)\n","        # Count of predictions that matched the target label\n","        correct_prediction += (prediction == labels).sum().item()\n","        total_prediction += prediction.shape[0]\n","\n","        if i % 50 == 0 and i != 0:    # print every 50 mini-batches\n","           print('At epoch {}, batch {}, having loss: {:.3f}'.format(epoch + 1, i + 1, running_loss / i))\n","    \n","    # Print stats at the end of the epoch\n","    \n","    num_batches = len(train_dl)\n","    avg_loss = running_loss / num_batches\n","    acc = correct_prediction/total_prediction\n","    training_loss_histroy.append(avg_loss)\n","    training_acc_history.append(acc)\n","    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.3f}, Training Accuracy: {acc:.3f}')\n","    \n","\n","    # if (epoch+1) % 5 == 0:\n","      # check the accuracy every 5 epoch\n","    inference(model, val_dl)\n","\n","  et_time = time.time()\n","  model_save_name = 'classifier_resnext101-unfreezed-100epooch.pt'\n","  path = F\"/content/drive/MyDrive/Colab Notebooks/SC201-Project/{model_save_name}\" \n","  torch.save(model.state_dict(), path)\n","  print('Finished Training')\n","  print('The training time is {:.3f} seconds'.format(et_time-st_time))\n","num_epochs= 30\n","training(train_dl, num_epochs, model)"],"execution_count":12,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Started Training\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["At epoch 1, batch 51, having loss: 0.036\n","At epoch 1, batch 101, having loss: 0.028\n","At epoch 1, batch 151, having loss: 0.028\n","At epoch 1, batch 201, having loss: 0.022\n","At epoch 1, batch 251, having loss: 0.021\n","At epoch 1, batch 301, having loss: 0.019\n","At epoch 1, batch 351, having loss: 0.019\n","At epoch 1, batch 401, having loss: 0.019\n","Epoch: 1, Loss: 0.019, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.021170 Total items: 1746\n","At epoch 2, batch 51, having loss: 0.027\n","At epoch 2, batch 101, having loss: 0.024\n","At epoch 2, batch 151, having loss: 0.021\n","At epoch 2, batch 201, having loss: 0.022\n","At epoch 2, batch 251, having loss: 0.024\n","At epoch 2, batch 301, having loss: 0.023\n","At epoch 2, batch 351, having loss: 0.021\n","At epoch 2, batch 401, having loss: 0.022\n","Epoch: 2, Loss: 0.022, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.024840 Total items: 1746\n","At epoch 3, batch 51, having loss: 0.035\n","At epoch 3, batch 101, having loss: 0.028\n","At epoch 3, batch 151, having loss: 0.025\n","At epoch 3, batch 201, having loss: 0.025\n","At epoch 3, batch 251, having loss: 0.023\n","At epoch 3, batch 301, having loss: 0.022\n","At epoch 3, batch 351, having loss: 0.022\n","At epoch 3, batch 401, having loss: 0.021\n","Epoch: 3, Loss: 0.020, Training Accuracy: 0.994\n","Validation Accuracy: 0.99, Validation Loss: 0.038230 Total items: 1746\n","At epoch 4, batch 51, having loss: 0.024\n","At epoch 4, batch 101, having loss: 0.030\n","At epoch 4, batch 151, having loss: 0.027\n","At epoch 4, batch 201, having loss: 0.028\n","At epoch 4, batch 251, having loss: 0.027\n","At epoch 4, batch 301, having loss: 0.024\n","At epoch 4, batch 351, having loss: 0.024\n","At epoch 4, batch 401, having loss: 0.023\n","Epoch: 4, Loss: 0.022, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.048949 Total items: 1746\n","At epoch 5, batch 51, having loss: 0.023\n","At epoch 5, batch 101, having loss: 0.018\n","At epoch 5, batch 151, having loss: 0.018\n","At epoch 5, batch 201, having loss: 0.020\n","At epoch 5, batch 251, having loss: 0.021\n","At epoch 5, batch 301, having loss: 0.026\n","At epoch 5, batch 351, having loss: 0.024\n","At epoch 5, batch 401, having loss: 0.024\n","Epoch: 5, Loss: 0.025, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.030654 Total items: 1746\n","At epoch 6, batch 51, having loss: 0.028\n","At epoch 6, batch 101, having loss: 0.026\n","At epoch 6, batch 151, having loss: 0.023\n","At epoch 6, batch 201, having loss: 0.023\n","At epoch 6, batch 251, having loss: 0.024\n","At epoch 6, batch 301, having loss: 0.023\n","At epoch 6, batch 351, having loss: 0.025\n","At epoch 6, batch 401, having loss: 0.023\n","Epoch: 6, Loss: 0.022, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.030362 Total items: 1746\n","At epoch 7, batch 51, having loss: 0.040\n","At epoch 7, batch 101, having loss: 0.033\n","At epoch 7, batch 151, having loss: 0.024\n","At epoch 7, batch 201, having loss: 0.029\n","At epoch 7, batch 251, having loss: 0.026\n","At epoch 7, batch 301, having loss: 0.026\n","At epoch 7, batch 351, having loss: 0.023\n","At epoch 7, batch 401, having loss: 0.025\n","Epoch: 7, Loss: 0.024, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.024227 Total items: 1746\n","At epoch 8, batch 51, having loss: 0.023\n","At epoch 8, batch 101, having loss: 0.032\n","At epoch 8, batch 151, having loss: 0.032\n","At epoch 8, batch 201, having loss: 0.032\n","At epoch 8, batch 251, having loss: 0.031\n","At epoch 8, batch 301, having loss: 0.029\n","At epoch 8, batch 351, having loss: 0.028\n","At epoch 8, batch 401, having loss: 0.029\n","Epoch: 8, Loss: 0.027, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.021816 Total items: 1746\n","At epoch 9, batch 51, having loss: 0.020\n","At epoch 9, batch 101, having loss: 0.030\n","At epoch 9, batch 151, having loss: 0.028\n","At epoch 9, batch 201, having loss: 0.027\n","At epoch 9, batch 251, having loss: 0.028\n","At epoch 9, batch 301, having loss: 0.026\n","At epoch 9, batch 351, having loss: 0.024\n","At epoch 9, batch 401, having loss: 0.024\n","Epoch: 9, Loss: 0.025, Training Accuracy: 0.994\n","Validation Accuracy: 0.99, Validation Loss: 0.039611 Total items: 1746\n","At epoch 10, batch 51, having loss: 0.023\n","At epoch 10, batch 101, having loss: 0.019\n","At epoch 10, batch 151, having loss: 0.020\n","At epoch 10, batch 201, having loss: 0.026\n","At epoch 10, batch 251, having loss: 0.029\n","At epoch 10, batch 301, having loss: 0.027\n","At epoch 10, batch 351, having loss: 0.026\n","At epoch 10, batch 401, having loss: 0.024\n","Epoch: 10, Loss: 0.024, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.028018 Total items: 1746\n","At epoch 11, batch 51, having loss: 0.020\n","At epoch 11, batch 101, having loss: 0.017\n","At epoch 11, batch 151, having loss: 0.016\n","At epoch 11, batch 201, having loss: 0.019\n","At epoch 11, batch 251, having loss: 0.019\n","At epoch 11, batch 301, having loss: 0.024\n","At epoch 11, batch 351, having loss: 0.024\n","At epoch 11, batch 401, having loss: 0.024\n","Epoch: 11, Loss: 0.024, Training Accuracy: 0.991\n","Validation Accuracy: 0.99, Validation Loss: 0.025096 Total items: 1746\n","At epoch 12, batch 51, having loss: 0.036\n","At epoch 12, batch 101, having loss: 0.035\n","At epoch 12, batch 151, having loss: 0.028\n","At epoch 12, batch 201, having loss: 0.023\n","At epoch 12, batch 251, having loss: 0.023\n","At epoch 12, batch 301, having loss: 0.023\n","At epoch 12, batch 351, having loss: 0.023\n","At epoch 12, batch 401, having loss: 0.023\n","Epoch: 12, Loss: 0.023, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.025271 Total items: 1746\n","At epoch 13, batch 51, having loss: 0.032\n","At epoch 13, batch 101, having loss: 0.027\n","At epoch 13, batch 151, having loss: 0.027\n","At epoch 13, batch 201, having loss: 0.024\n","At epoch 13, batch 251, having loss: 0.023\n","At epoch 13, batch 301, having loss: 0.022\n","At epoch 13, batch 351, having loss: 0.022\n","At epoch 13, batch 401, having loss: 0.023\n","Epoch: 13, Loss: 0.023, Training Accuracy: 0.991\n","Validation Accuracy: 0.99, Validation Loss: 0.038466 Total items: 1746\n","At epoch 14, batch 51, having loss: 0.021\n","At epoch 14, batch 101, having loss: 0.022\n","At epoch 14, batch 151, having loss: 0.021\n","At epoch 14, batch 201, having loss: 0.020\n","At epoch 14, batch 251, having loss: 0.022\n","At epoch 14, batch 301, having loss: 0.023\n","At epoch 14, batch 351, having loss: 0.023\n","At epoch 14, batch 401, having loss: 0.027\n","Epoch: 14, Loss: 0.026, Training Accuracy: 0.991\n","Validation Accuracy: 0.99, Validation Loss: 0.025798 Total items: 1746\n","At epoch 15, batch 51, having loss: 0.024\n","At epoch 15, batch 101, having loss: 0.023\n","At epoch 15, batch 151, having loss: 0.026\n","At epoch 15, batch 201, having loss: 0.027\n","At epoch 15, batch 251, having loss: 0.024\n","At epoch 15, batch 301, having loss: 0.025\n","At epoch 15, batch 351, having loss: 0.024\n","At epoch 15, batch 401, having loss: 0.024\n","Epoch: 15, Loss: 0.025, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.031636 Total items: 1746\n","At epoch 16, batch 51, having loss: 0.012\n","At epoch 16, batch 101, having loss: 0.017\n","At epoch 16, batch 151, having loss: 0.022\n","At epoch 16, batch 201, having loss: 0.020\n","At epoch 16, batch 251, having loss: 0.022\n","At epoch 16, batch 301, having loss: 0.021\n","At epoch 16, batch 351, having loss: 0.022\n","At epoch 16, batch 401, having loss: 0.023\n","Epoch: 16, Loss: 0.024, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.046393 Total items: 1746\n","At epoch 17, batch 51, having loss: 0.011\n","At epoch 17, batch 101, having loss: 0.021\n","At epoch 17, batch 151, having loss: 0.019\n","At epoch 17, batch 201, having loss: 0.020\n","At epoch 17, batch 251, having loss: 0.020\n","At epoch 17, batch 301, having loss: 0.019\n","At epoch 17, batch 351, having loss: 0.019\n","At epoch 17, batch 401, having loss: 0.019\n","Epoch: 17, Loss: 0.019, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.025862 Total items: 1746\n","At epoch 18, batch 51, having loss: 0.014\n","At epoch 18, batch 101, having loss: 0.022\n","At epoch 18, batch 151, having loss: 0.020\n","At epoch 18, batch 201, having loss: 0.021\n","At epoch 18, batch 251, having loss: 0.021\n","At epoch 18, batch 301, having loss: 0.019\n","At epoch 18, batch 351, having loss: 0.020\n","At epoch 18, batch 401, having loss: 0.019\n","Epoch: 18, Loss: 0.019, Training Accuracy: 0.994\n","Validation Accuracy: 0.99, Validation Loss: 0.025567 Total items: 1746\n","At epoch 19, batch 51, having loss: 0.040\n","At epoch 19, batch 101, having loss: 0.031\n","At epoch 19, batch 151, having loss: 0.027\n","At epoch 19, batch 201, having loss: 0.022\n","At epoch 19, batch 251, having loss: 0.019\n","At epoch 19, batch 301, having loss: 0.021\n","At epoch 19, batch 351, having loss: 0.023\n","At epoch 19, batch 401, having loss: 0.022\n","Epoch: 19, Loss: 0.021, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.030575 Total items: 1746\n","At epoch 20, batch 51, having loss: 0.016\n","At epoch 20, batch 101, having loss: 0.018\n","At epoch 20, batch 151, having loss: 0.022\n","At epoch 20, batch 201, having loss: 0.019\n","At epoch 20, batch 251, having loss: 0.019\n","At epoch 20, batch 301, having loss: 0.017\n","At epoch 20, batch 351, having loss: 0.016\n","At epoch 20, batch 401, having loss: 0.018\n","Epoch: 20, Loss: 0.017, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.024694 Total items: 1746\n","At epoch 21, batch 51, having loss: 0.026\n","At epoch 21, batch 101, having loss: 0.016\n","At epoch 21, batch 151, having loss: 0.017\n","At epoch 21, batch 201, having loss: 0.019\n","At epoch 21, batch 251, having loss: 0.019\n","At epoch 21, batch 301, having loss: 0.020\n","At epoch 21, batch 351, having loss: 0.024\n","At epoch 21, batch 401, having loss: 0.025\n","Epoch: 21, Loss: 0.024, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.029158 Total items: 1746\n","At epoch 22, batch 51, having loss: 0.018\n","At epoch 22, batch 101, having loss: 0.018\n","At epoch 22, batch 151, having loss: 0.020\n","At epoch 22, batch 201, having loss: 0.019\n","At epoch 22, batch 251, having loss: 0.019\n","At epoch 22, batch 301, having loss: 0.019\n","At epoch 22, batch 351, having loss: 0.020\n","At epoch 22, batch 401, having loss: 0.020\n","Epoch: 22, Loss: 0.020, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.031062 Total items: 1746\n","At epoch 23, batch 51, having loss: 0.021\n","At epoch 23, batch 101, having loss: 0.017\n","At epoch 23, batch 151, having loss: 0.023\n","At epoch 23, batch 201, having loss: 0.021\n","At epoch 23, batch 251, having loss: 0.021\n","At epoch 23, batch 301, having loss: 0.022\n","At epoch 23, batch 351, having loss: 0.024\n","At epoch 23, batch 401, having loss: 0.024\n","Epoch: 23, Loss: 0.024, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.032305 Total items: 1746\n","At epoch 24, batch 51, having loss: 0.025\n","At epoch 24, batch 101, having loss: 0.021\n","At epoch 24, batch 151, having loss: 0.024\n","At epoch 24, batch 201, having loss: 0.020\n","At epoch 24, batch 251, having loss: 0.019\n","At epoch 24, batch 301, having loss: 0.022\n","At epoch 24, batch 351, having loss: 0.021\n","At epoch 24, batch 401, having loss: 0.019\n","Epoch: 24, Loss: 0.020, Training Accuracy: 0.992\n","Validation Accuracy: 0.99, Validation Loss: 0.021245 Total items: 1746\n","At epoch 25, batch 51, having loss: 0.014\n","At epoch 25, batch 101, having loss: 0.017\n","At epoch 25, batch 151, having loss: 0.018\n","At epoch 25, batch 201, having loss: 0.018\n","At epoch 25, batch 251, having loss: 0.018\n","At epoch 25, batch 301, having loss: 0.018\n","At epoch 25, batch 351, having loss: 0.018\n","At epoch 25, batch 401, having loss: 0.017\n","Epoch: 25, Loss: 0.017, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.021753 Total items: 1746\n","At epoch 26, batch 51, having loss: 0.013\n","At epoch 26, batch 101, having loss: 0.016\n","At epoch 26, batch 151, having loss: 0.016\n","At epoch 26, batch 201, having loss: 0.016\n","At epoch 26, batch 251, having loss: 0.016\n","At epoch 26, batch 301, having loss: 0.016\n","At epoch 26, batch 351, having loss: 0.017\n","At epoch 26, batch 401, having loss: 0.017\n","Epoch: 26, Loss: 0.017, Training Accuracy: 0.994\n","Validation Accuracy: 0.99, Validation Loss: 0.026824 Total items: 1746\n","At epoch 27, batch 51, having loss: 0.022\n","At epoch 27, batch 101, having loss: 0.018\n","At epoch 27, batch 151, having loss: 0.022\n","At epoch 27, batch 201, having loss: 0.021\n","At epoch 27, batch 251, having loss: 0.019\n","At epoch 27, batch 301, having loss: 0.021\n","At epoch 27, batch 351, having loss: 0.020\n","At epoch 27, batch 401, having loss: 0.020\n","Epoch: 27, Loss: 0.020, Training Accuracy: 0.994\n","Validation Accuracy: 0.99, Validation Loss: 0.023578 Total items: 1746\n","At epoch 28, batch 51, having loss: 0.029\n","At epoch 28, batch 101, having loss: 0.022\n","At epoch 28, batch 151, having loss: 0.022\n","At epoch 28, batch 201, having loss: 0.022\n","At epoch 28, batch 251, having loss: 0.019\n","At epoch 28, batch 301, having loss: 0.021\n","At epoch 28, batch 351, having loss: 0.019\n","At epoch 28, batch 401, having loss: 0.018\n","Epoch: 28, Loss: 0.020, Training Accuracy: 0.993\n","Validation Accuracy: 0.99, Validation Loss: 0.015463 Total items: 1746\n","At epoch 29, batch 51, having loss: 0.022\n","At epoch 29, batch 101, having loss: 0.020\n","At epoch 29, batch 151, having loss: 0.016\n","At epoch 29, batch 201, having loss: 0.015\n","At epoch 29, batch 251, having loss: 0.014\n","At epoch 29, batch 301, having loss: 0.014\n","At epoch 29, batch 351, having loss: 0.014\n","At epoch 29, batch 401, having loss: 0.016\n","Epoch: 29, Loss: 0.016, Training Accuracy: 0.994\n","Validation Accuracy: 0.99, Validation Loss: 0.023108 Total items: 1746\n","At epoch 30, batch 51, having loss: 0.007\n","At epoch 30, batch 101, having loss: 0.012\n","At epoch 30, batch 151, having loss: 0.012\n","At epoch 30, batch 201, having loss: 0.012\n","At epoch 30, batch 251, having loss: 0.016\n","At epoch 30, batch 301, having loss: 0.017\n","At epoch 30, batch 351, having loss: 0.017\n","At epoch 30, batch 401, having loss: 0.016\n","Epoch: 30, Loss: 0.017, Training Accuracy: 0.995\n","Validation Accuracy: 0.99, Validation Loss: 0.018910 Total items: 1746\n","Finished Training\n","The training time is 30133.944 seconds\n"]}]},{"cell_type":"code","metadata":{"id":"9nNQ3q2U683S"},"source":["def inference (model, val_dl):\n","  correct_prediction_val = 0\n","  total_prediction_val = 0\n","  running_loss_val = 0\n","\n","  # Disable gradient updates\n","  with torch.no_grad():\n","    for data in val_dl:\n","      criterion = nn.CrossEntropyLoss()\n","      # Get the input features and target labels, and put them on the GPU\n","      inputs, labels = data[0].to(device), data[1].to(device)\n","\n","      # Normalize the inputs\n","      inputs_m, inputs_s = inputs.mean(), inputs.std()\n","      inputs = (inputs - inputs_m) / inputs_s\n","\n","      # Get predictions\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      running_loss_val += loss.item()\n","      # Get the predicted class with the highest score\n","      _, prediction = torch.max(outputs,1)\n","      # Count of predictions that matched the target label\n","      correct_prediction_val += (prediction == labels).sum().item()\n","      total_prediction_val += prediction.shape[0]\n","  num_batches = len(val_dl)\n","  avg_loss = running_loss_val / num_batches\n","  acc = correct_prediction_val/total_prediction_val\n","  validation_loss_history.append(avg_loss)\n","  validation_acc_history.append(acc)\n","    \n","  print(f'Validation Accuracy: {acc:.2f}, Validation Loss: {running_loss_val/num_batches:3f} Total items: {total_prediction_val}')\n","\n","# Run inference on trained model with the validation set\n","# inference(myModel, val_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"ovwBzQu2jWs7","executionInfo":{"status":"ok","timestamp":1630634389895,"user_tz":240,"elapsed":254,"user":{"displayName":"Jason Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqBrnZeHswKzAdjSeDicT44J2-GHR4HjZAiINH8eY=s64","userId":"01645798857031101691"}},"outputId":"b908d3cf-a1c1-4132-a245-a3f0641c0517"},"source":["import pandas as pd\n","from google.colab import files\n","pd.DataFrame(validation_loss_history).to_csv('validation_loss_history_Resnet_unfreezed.csv')\n","pd.DataFrame(validation_acc_history).to_csv('validation_acc_history_Resnet_unfreezed.csv')\n","pd.DataFrame(training_loss_histroy).to_csv('training_loss_histroy_Resnet_unfreezed.csv')\n","pd.DataFrame(training_acc_history).to_csv('training_acc_histroy_Resnet_unfreezed.csv')\n","files.download('validation_loss_history_Resnet_unfreezed.csv')\n","files.download('validation_acc_history_Resnet_unfreezed.csv')\n","files.download('training_loss_histroy_Resnet_unfreezed.csv')\n","files.download('training_acc_histroy_Resnet_unfreezed.csv')\n","print(validation_loss_history)\n","print(validation_acc_history)\n","print(training_loss_histroy)\n","print(training_acc_history)\n"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_74cbebbd-45fd-4abd-a16e-8341c61d7bbd\", \"validation_loss_history_Resnet_unfreezed.csv\", 700)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_977662fe-21d9-4657-ae90-10646c51ac7c\", \"validation_acc_history_Resnet_unfreezed.csv\", 651)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_c1ae9e62-d622-4761-8b9d-bb15c7afa9e6\", \"training_loss_histroy_Resnet_unfreezed.csv\", 700)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_80b8059b-2d65-448d-8c09-06820ced6b2c\", \"training_acc_histroy_Resnet_unfreezed.csv\", 651)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[0.02116993127009385, 0.024839670180530633, 0.03822971842346202, 0.04894922624962832, 0.030653590066530838, 0.030361600962406886, 0.02422721373170382, 0.021815956446014504, 0.03961132439127885, 0.028017719313010565, 0.025096471600677846, 0.025271171618804933, 0.038465522662636434, 0.025798433198059974, 0.03163592282706056, 0.046393437248240264, 0.025862392508208192, 0.025566795499515717, 0.03057516650620032, 0.024693567576181605, 0.02915768568463012, 0.031062282665846975, 0.0323049977648372, 0.02124520076220981, 0.021752606817326058, 0.026824195344512313, 0.02357776487833896, 0.015463281511920906, 0.02310781331372337, 0.018910101449428524]\n","[0.9936998854524628, 0.9914089347079038, 0.9896907216494846, 0.9891179839633448, 0.9908361970217641, 0.9902634593356243, 0.9908361970217641, 0.9925544100801833, 0.9873997709049256, 0.9936998854524628, 0.9936998854524628, 0.9925544100801833, 0.9908361970217641, 0.993127147766323, 0.9885452462772051, 0.9914089347079038, 0.9936998854524628, 0.9908361970217641, 0.9902634593356243, 0.9914089347079038, 0.9914089347079038, 0.9919816723940436, 0.9885452462772051, 0.9914089347079038, 0.9942726231386025, 0.9908361970217641, 0.993127147766323, 0.9919816723940436, 0.9914089347079038, 0.9942726231386025]\n","[0.019451702472225604, 0.022227191559677048, 0.019834826451177495, 0.02163682653959432, 0.024611863787943503, 0.022152875389723103, 0.024435197221397944, 0.027427554382388235, 0.02513206250804598, 0.024379037290492834, 0.02384331104102893, 0.02313540704833639, 0.02337198107705367, 0.02615422804780552, 0.02467403134470102, 0.024319809491685093, 0.01850833485678549, 0.01884786296726008, 0.021461851458674613, 0.017450624511058763, 0.024128352946221853, 0.020258700944800898, 0.023777618277454466, 0.0199764113316733, 0.016952994654968682, 0.017119194825877128, 0.02015498598627496, 0.01977602102146478, 0.016118262550234408, 0.017430513584705674]\n","[0.9934154022330375, 0.9934154022330375, 0.9941311193816204, 0.9934154022330375, 0.9919839679358717, 0.9918408245061552, 0.9931291153736044, 0.9918408245061552, 0.993558545662754, 0.9921271113655883, 0.9914113942170054, 0.9929859719438878, 0.9914113942170054, 0.9914113942170054, 0.9924133982250215, 0.9926996850844546, 0.9929859719438878, 0.9939879759519038, 0.9934154022330375, 0.9932722588033209, 0.9921271113655883, 0.9929859719438878, 0.9919839679358717, 0.9924133982250215, 0.9929859719438878, 0.993558545662754, 0.9941311193816204, 0.9932722588033209, 0.9941311193816204, 0.9947036931004867]\n"]}]},{"cell_type":"code","metadata":{"id":"hbiUiVGqiTu2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6UjGbJ1bD6L"},"source":["from torchvision import models\n","from torchsummary import summary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YeZbqXUCbHLQ"},"source":["model = models.resnext101_32x8d(pretrained=True)\n","for param in model.parameters():\n","  param.requires_grad = False\n","model = nn.Sequential(\n","  nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n","  model)\n","model.fc = nn.Sequential(\n","    nn.Linear(1000,10,bias=True))\n","\n","for param in model.parameters():\n","  print(param.requires_grad)\n","summary(model,(2,64,64))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsUg_zvqhMF0"},"source":["model = models.resnext101_32x8d(pretrained=True)\n","summary(model, (3,64,64))"],"execution_count":null,"outputs":[]}]}